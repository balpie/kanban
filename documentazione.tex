\documentclass{article}

\usepackage[italian]{babel}
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\setcounter{secnumdepth}{0}

\begin{document}

% ----- Intestazione -----
\begin{flushleft}
\normalsize
kanban -- Pietro Balestri -- 03/01/2026\\
\rule{\textwidth}{0.4pt}
\end{flushleft}

\section{Introduzione}
Per la gestione delle connessioni è stato adottato un approccio multithreading. 
In questo modo viene garantito l'isolamento della comunicazione con i client, 
e in caso di operazioni bloccanti di un thread, gli altri possono continuare ad effettuare le operazioni possibili.
L'approccio multiprocesso è stato scartato per via della minore praticità nella gestione di strutture dati comuni ai vari thread.
Un thread per ogni client servito implica comunque un overhead di memoria, e inoltre la gestione delle strutture dati concorrenti in modo thread-safe può risultare non triviale. 
In questo documento, per evitare ambiguità, \textit{lavagna} indicherà la struttura dati contenente le card suddivise in colonne, mentre \textit{server} indicherà il processo corrispondente all'eseguibile di nome \verb|lavagna|
\subsection{Compilazione}
Per compilare sia il binario della lavagna che quello degli utenti è sufficiente chiamare \verb|make| a partire dalla directory del 
progetto.
\subsection{Filetree}
I file sorgente sono all'interno della cartella \verb|src|. 
Il codice è stato diviso in 3 gruppi: codice comune, codice di lavagna, e codice di utente. Per ognuno dei gruppi c'è un ulteriore divisione tra codice di rete e non.
I binari compariranno nella cartella \verb|bin| dopo la compilazione.
\section{Strutture Dati}
\subsection{Server}
Il server mantiene socket, porta, e indirizzo di tutti gli utenti attualmente connessi all'interno di una linked-list di connessioni.
Anche la lavagna è una linked list di card, e le due strutture sono separate. 
La struttura dati delle connessioni è protetta da mutex, mentre lavagna è protetta da rwlock, per permettere che più letture possano essere eseguite concorrentemente. 
L'assenza di un collegamento tra le card della lavagna e le connessioni, garantisce maggiore flessibilità. Per esempio, un utente può creare una card da mettere nella colonna done, nonostante l'attuale assenza dell'utente che l'ha compiuta.
Per garantire una gestione sincrona delle aste a parte del server, vengono usate due condition variable: una per il termine dell'asta, e una per l'inizio.
\subsection{Utente}
Anche l'utente rappresenta la lavagna come una linked list, ma la richiede al server ogni volta che viene visualizzata. Seppure questo approccio si appoggia alle risorse di rete e introduce un certo grado di ridondanza, elimina completamente il problema di mantenere la lavagna sincronizzata in ogni utente. 
Lo stesso ragionamento è stato applicato alla lista di peer, richiesta al server all'inizio di ogni asta.
Ogni utente, in oltre, ha una lista di card, con all'interno tutte le proprie card in doing. Questa è gestita autonomamente, e non deve essere necessariamente sincronizzata con la lavagna del server. Nonostante questo permette teoricamente la presenza di inconsistenze, queste non verrebbero propagate al server, per via della sanificazione delle richieste effettuata in esso.
L'utente è composto da tre thread: uno che si occupa della comunicazione con la lavagna e con gli altri peer, uno che si occupa dell'input-output su terminale, e uno che si occupa di fare le attività richieste dalle card (via sleep).
Per coordinare i tre, l'eseguibile utente si appoggia a tre strutture dati aggiuntive: \verb|created|, \verb|message_queue|, e \verb|worker_occupato|. La prima è un puntatore alla card creata dall'utente, utile per evitare race condition tra la sua creazione e l'invio. 
La seconda è una coda circolare di messaggi raccolti dal thread I/O e inviati al server dal thread che si occupa della comunicazione. 
La terza è una variabile booleana. 
Tutte e tre le strutture sono protette da mutex
\section{Flusso di comunicazione}
Due eventi distinti devono poter dare inizio a un'asta: 
una \verb|CREATE_CARD|, con colonna to-do, nel caso in cui ci fossero già due utenti connessi al server, e
la connessione del secondo utente, con una o più carte presenti nella colonna to-do.
Questo impone che debba essere possibile sia che l'utente comunichi al server una nuova card, che il server possa richiedere agli utenti di effettuare un'asta, in qualsiasi momento. 
Per fare fronte a questa esigenza si è optato per assegnare un timeout alla \verb|recv| sia nel server che in utente, di fatto implementando polling con timeout da entrambe le parti.
In una prima implementazione la soluzione trovata era quella di inviare avanti e indietro un messaggio con significato "nessuna istruzione" nei momenti di inattività, ma questa opzione è stata scartata in quanto costituiva attesa attiva, e aggiungeva carico inutile alla rete.
Era possibile anche utilizzare la \verb|select| con timeout, ma anche quest'opzione è stata scartata, vista la maggiore complessità e minore leggibilità. 
\subsection{Messaggi Server-Utenti}
I messaggi sono contraddistinti da un header, di due byte, e opzionalmente da un payload.
Nell'header il primo byte è un codice che indica l'operazione richiesta e implicitamente il significato del secondo byte. 
Quest'ultimo quasi sempre è una dimensione, e se non lo è, non è significativo.
Un protocollo così strutturato garantisce estensibilità: è possibile aggiungere un'operazione semplicemente assegnandogli un codice su 8 bit. 
La quantità di possibili operazioni è limitata a 256, ma questo è stato ritenuto un numero sufficiente per l'applicazione.
Il fatto che i codici di operazioni siano numerici e non mnemonici rende l'applicazione meno leggibile da esseri umani, ma il problema è mitigato dalla natura semplice delle intereazioni tra server e utenti.
Per lo scambio delle card, il protocollo implementato è di tipo binario.
Un protocollo testuale è stato escluso a priori, per le seguenti ragioni:
Il costo della serializzazione e deserializzazione dei timestamp, ma anche dei numeri di porta, è stato considerato inferiore rispetto al costo della formattazione e del parsing delle relative stringhe.
In oltre, soprattutto per i timestamp, sarebbe stato necessario inviare più byte all'interno dei messaggi rispetto che nella soluzione adottata.

\subsection{Messaggi Utenti-Utenti}
Gli utenti inviano tra loro esclusivamente il proprio costo relativo all'asta corrente, su un singolo byte. L'invio avviene in ordine di porta: l'utente con la porta più alta è il primo ad inviare il proprio costo a tutti gli altri, e così via. Le connessioni sono iniziate da chi deve inviare il messaggio, e vengono chiuse al termine di ogni asta e riaperte all'inizio.
Una criticità del protocollo è la ridondaza: con $n$ client, i relativi messaggi saranno $O(n^2)$.
Inoltre, in questa specifica implementazione del protocollo, le continue connessioni e disconnessioni TCP possono costituire uno spreco di risorse, per via del triplice handshake iniziale e delle risorse ad esse allocate. Per ovviare a questo problema si potrebbe implementare una lista persistente di peer, modificata solo nel caso di una disconnessione, o dell'arrivo di un nuovo peer.
La complessità aggiunta di questa soluzione è stata valutata superare i relativi benefici, quindi l'idea è stata scartata. 
Come meccanismo di fallback, è sato dato ai socket dei peer, sia per la ricezione che per l'invio dei messaggi, un timeout, in modo da evitare deadlock in caso di disconnessione inaspettata di un peer durante l'asta.
\vfill
\hrule
\vspace{0.5em}
{\footnotesize
Nella consegna del progetto è incluso lo script in bash \verb|usr.sh|, che equivale ad eseguire i client, passando come porta la successiva porta libera, a partire da \verb|5679|, oppure l'ultima porta utilizzata dallo script.
Inoltre, i file di log sono pensati per essere mostrati a video con \verb|cat|
}
\end{document}
